# FOUNDER 研究笔记

**论文**: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making  
**会议**: ICML 2025  
**项目页**: https://sites.google.com/view/founder-rl  
**论文**: https://openreview.net/forum?id=UTT5OTyIWm

---

## 核心贡献

FOUNDER将**基础模型植根于世界模型**中，实现开放式具身决策。这是将大语言模型/视觉-语言模型与世界模型深度结合的重要工作。

---

## 核心创新：时间距离奖励 (Temporal Distance Reward)

### 问题：GenRL的局限性

**GenRL的失败案例**（FOUNDER的前身方法）：

1. **视觉层面对齐问题**
   - GenRL仅依赖随机状态序列进行连接器和策略学习
   - 随机状态只包含单个视觉观察的信息
   - 限制了GenRL只能进行视觉层面对齐

2. **距离度量问题**
   - GenRL使用随机状态之间的余弦相似度作为伪奖励
   - 但随机状态是分类变量（类似DreamerV2/V3），无法直接计算
   - 解决方法：将随机状态前向传播到图像解码器，使用解码器第一层线性层的密集向量计算余弦相似度
   - **问题加剧**：图像解码器输出主要包含视觉信息，尤其当解码器不以确定性状态为条件时

3. **跨域任务失败**
   - 跨具身/跨视角场景（深层任务语义未提取）
   - 复杂视觉观察任务（如Minecraft，复杂观察的视觉对齐困难）

### FOUNDER的解决方案

#### 1. 世界模型状态改进

使用**确定性状态 + 随机状态**的组合：
- 确定性状态：包含历史信息和任务语义
- 随机状态：包含当前观察信息

#### 2. 时间距离奖励函数 (TempD)

**核心思想**：使用时间距离作为奖励函数，而非直接的余弦相似度或KL散度。

**FOUNDER w/o TempD的失败**：
- 使用余弦相似度作为距离度量
- 计算确定性和随机状态各自的余弦距离之和
- **奖励黑客问题**：真实回报和伪奖励曲线呈完全相反趋势
  - 智能体在最大化伪奖励时，真实性能反而恶化
  - 例如：Walker Run任务中，智能体在原地"跑步"，视觉上像在跑，但实际没有前进

**失败案例分析**（Walker Run, Cheetah Run, Stickman Walk）：
- 初期表现良好，但随着训练进行性能恶化
- 最终智能体行为变得静态
- 轨迹显示：智能体停留在初始位置，表现出"跑步"或"行走"的样子，但实际上只是在原地运动或以极慢速度前进

**根本原因**：
- 基于余弦相似度或其他直接距离度量的奖励函数导致策略模仿智能体的视觉外观
- 忽略了底层任务语义和多步运动
- 缺乏时间感知和关键任务完成信息

**FOUNDER的时间距离方法**：
- 利用时间距离作为奖励函数
- 消除奖励黑客问题（真实回报和伪奖励曲线趋势一致）
- 伪奖励 = 预测的目标状态与当前状态之间的时间距离 + 1
- 增强时间感知，融入更多任务完成信息

---

## 技术架构

### 三阶段学习范式

1. **世界模型学习**：学习环境动力学
2. **连接器学习**：将基础模型输出映射到世界模型状态
3. **行为学习**：基于时间距离奖励学习策略

### vs GenRL

| 维度 | GenRL | FOUNDER |
|------|-------|---------|
| **状态表示** | 仅随机状态 | 确定性 + 随机状态 |
| **对齐层次** | 视觉层面 | 深层语义 |
| **距离度量** | 余弦相似度 | 时间距离 |
| **跨域能力** | 弱 | 强 |
| **奖励一致性** | 差（奖励黑客） | 好 |

---

## 实验结果

### 成功案例

**Kitchen任务**（Franka Kitchen）：
- Kitchen Microwave
- Kitchen Light
- Kitchen Slide
- Kitchen Burner

FOUNDER成功完成所有任务，而GenRL失败。

**失败原因分析**（GenRL）：
- 智能体机械地模仿目标状态序列的视觉特征
- 未完全掌握任务的深层语义
- 例如：能够移动到正确位置，但无法完成最终操作（如打开微波炉门、操作滑动柜、转动旋钮）

### 跨视角任务

**Cheetah Run**（跨视角视频提示）：
- GenRL生成的轨迹：猎豹视觉上倾斜，但实际没有奔跑
- FOUNDER成功：猎豹开始奔跑，提取了深层任务语义

---

## 关键洞察

1. **时间距离是关键**：相比直接距离度量，时间距离能更好地捕捉任务完成的动态过程
2. **深层语义对齐**：不能仅停留在视觉层面，需要提取任务的深层语义
3. **奖励设计的重要性**：不当的奖励函数会导致奖励黑客问题
4. **状态表示的丰富性**：确定性状态包含历史和语义信息，对任务成功至关重要

---

## 对具身智能的意义

1. **开放式决策**：支持基于自然语言/视频的开放式任务指定
2. **跨域泛化**：通过深层语义对齐实现跨具身、跨视角泛化
3. **基础模型集成**：为将大模型能力引入具身智能提供有效范式
4. **避免奖励黑客**：时间距离方法为奖励设计提供新思路
